{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "7IF-QzLDzEZR",
        "ENfU-i_wzMDu",
        "a7jHY3mZUOhV"
      ],
      "authorship_tag": "ABX9TyM/AWJdMuc4TwJiQoFxeev+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UmarBalak/Federated-Learning-with-TensorFlow/blob/main/TensorFlow_Fedetared_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rmyWch53NpLO",
        "outputId": "87c68086-aa4b-44dd-ee51-41d6b002bfbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_federated in /usr/local/lib/python3.10/dist-packages (0.79.0)\n",
            "Requirement already satisfied: absl-py==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.4.0)\n",
            "Requirement already satisfied: attrs~=23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (23.1.0)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (5.3.3)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (0.1.8)\n",
            "Requirement already satisfied: dp-accounting==0.4.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (0.4.3)\n",
            "Requirement already satisfied: farmhashpy==0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (0.4.0)\n",
            "Requirement already satisfied: google-vizier==0.1.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (0.1.11)\n",
            "Requirement already satisfied: grpcio~=1.46 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.64.0)\n",
            "Requirement already satisfied: jaxlib==0.4.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (0.4.14)\n",
            "Requirement already satisfied: jax==0.4.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (0.4.14)\n",
            "Requirement already satisfied: numpy~=1.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.25.2)\n",
            "Requirement already satisfied: portpicker~=1.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.6.0)\n",
            "Requirement already satisfied: scipy~=1.9.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.9.3)\n",
            "Requirement already satisfied: semantic-version~=2.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-model-optimization==0.7.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (0.7.5)\n",
            "Requirement already satisfied: tensorflow-privacy==0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (0.9.0)\n",
            "Requirement already satisfied: tensorflow==2.14.*,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (2.14.1)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions==4.5.*,>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (4.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos==1.61.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_federated) (1.61.0)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.10/dist-packages (from dp-accounting==0.4.3->tensorflow_federated) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow_federated) (4.25.3)\n",
            "Requirement already satisfied: grpcio-tools>=1.35.0 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow_federated) (1.62.2)\n",
            "Requirement already satisfied: sqlalchemy<=1.4.20,>=1.4 in /usr/local/lib/python3.10/dist-packages (from google-vizier==0.1.11->tensorflow_federated) (1.4.20)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14->tensorflow_federated) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.14->tensorflow_federated) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (18.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (22.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.37.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.14.0)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy==0.9.0->tensorflow_federated) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy==0.9.0->tensorflow_federated) (0.22.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow_federated) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow_federated) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker~=1.6->tensorflow_federated) (5.9.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.43.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow_federated) (3.0.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.0.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow_federated) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow_federated) (2.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.*,>=2.14.0->tensorflow_federated) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_federated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "Hoaa1EXkv9m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***MNIST***"
      ],
      "metadata": {
        "id": "7IF-QzLDzEZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()"
      ],
      "metadata": {
        "id": "RoRodDNiwUrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(emnist_train.client_ids)"
      ],
      "metadata": {
        "id": "oCSDebG4xdhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emnist_train.element_type_structure"
      ],
      "metadata": {
        "id": "06PWbjihxgBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "    emnist_train.client_ids[0])\n",
        "\n",
        "example_element = next(iter(example_dataset))\n",
        "\n",
        "example_element['label'].numpy()"
      ],
      "metadata": {
        "id": "QBAjSkWjxg7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(example_element['pixels'].numpy(), cmap='gray', aspect='equal')\n",
        "plt.grid(False)\n",
        "_ = plt.show()"
      ],
      "metadata": {
        "id": "jDTDkjpbxg_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Example MNIST digits for one client\n",
        "figure = plt.figure(figsize=(20, 4))\n",
        "j = 0\n",
        "\n",
        "for example in example_dataset.take(40):\n",
        "  plt.subplot(4, 10, j+1)\n",
        "  plt.imshow(example['pixels'].numpy(), cmap='gray', aspect='equal')\n",
        "  plt.axis('off')\n",
        "  j += 1"
      ],
      "metadata": {
        "id": "WCFfCPI6xhCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of examples per layer for a sample of clients\n",
        "f = plt.figure(figsize=(12, 7))\n",
        "f.suptitle('Label Counts for a Sample of Clients')\n",
        "for i in range(6):\n",
        "  client_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "      emnist_train.client_ids[i])\n",
        "  plot_data = collections.defaultdict(list)\n",
        "  for example in client_dataset:\n",
        "    # Append counts individually per label to make plots\n",
        "    # more colorful instead of one color per plot.\n",
        "    label = example['label'].numpy()\n",
        "    plot_data[label].append(label)\n",
        "  plt.subplot(2, 3, i+1)\n",
        "  plt.title('Client {}'.format(i))\n",
        "  for j in range(10):\n",
        "    plt.hist(\n",
        "        plot_data[j],\n",
        "        density=False,\n",
        "        bins=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
      ],
      "metadata": {
        "id": "cbc3ZRwuxpsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each client has different mean images, meaning each client will be nudging\n",
        "# the model in their own directions locally.\n",
        "\n",
        "for i in range(5):\n",
        "  client_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "      emnist_train.client_ids[i])\n",
        "  plot_data = collections.defaultdict(list)\n",
        "  for example in client_dataset:\n",
        "    plot_data[example['label'].numpy()].append(example['pixels'].numpy())\n",
        "  f = plt.figure(i, figsize=(12, 5))\n",
        "  f.suptitle(\"Client #{}'s Mean Image Per Label\".format(i))\n",
        "  for j in range(10):\n",
        "    mean_img = np.mean(plot_data[j], 0)\n",
        "    plt.subplot(2, 5, j+1)\n",
        "    plt.imshow(mean_img.reshape((28, 28)))\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "xQDqR6H1xpvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLIENTS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "  def batch_format_fn(element):\n",
        "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.reshape(element['pixels'], [-1, 784]),\n",
        "        y=tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(\n",
        "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
      ],
      "metadata": {
        "id": "SyCfAZ15xpzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_example_dataset = preprocess(example_dataset)\n",
        "\n",
        "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
        "                                     next(iter(preprocessed_example_dataset)))\n",
        "\n",
        "sample_batch"
      ],
      "metadata": {
        "id": "9rJhWYS1xp4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in client_ids\n",
        "  ]"
      ],
      "metadata": {
        "id": "OUmVYEQfx40b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_clients = emnist_train.client_ids[0:NUM_CLIENTS]\n",
        "\n",
        "federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "\n",
        "print(f'Number of client datasets: {len(federated_train_data)}')\n",
        "print(f'First dataset: {federated_train_data[0]}')"
      ],
      "metadata": {
        "id": "s24DqYENx43W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_keras_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(784,)),\n",
        "      tf.keras.layers.Dense(10, kernel_initializer='zeros'),\n",
        "      tf.keras.layers.Softmax(),\n",
        "  ])"
      ],
      "metadata": {
        "id": "r1fsklxwx45t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn():\n",
        "  # We _must_ create a new model here, and _not_ capture it from an external\n",
        "  # scope. TFF will call this within different graph contexts.\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=preprocessed_example_dataset.element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "OUYIJHpexhHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))"
      ],
      "metadata": {
        "id": "0MkhQu26yGEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_process.initialize.type_signature.formatted_representation())"
      ],
      "metadata": {
        "id": "IGSXlQA8yGHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_state = training_process.initialize()"
      ],
      "metadata": {
        "id": "ohedp2YtyGKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = training_process.next(train_state, federated_train_data)\n",
        "train_state = result.state\n",
        "train_metrics = result.metrics\n",
        "print('round  1, metrics={}'.format(train_metrics))"
      ],
      "metadata": {
        "id": "wjTZA6N_yGM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_ROUNDS = 11\n",
        "for round_num in range(2, NUM_ROUNDS):\n",
        "  result = training_process.next(train_state, federated_train_data)\n",
        "  train_state = result.state\n",
        "  train_metrics = result.metrics\n",
        "  print('round {:2d}, metrics={}'.format(round_num, train_metrics))"
      ],
      "metadata": {
        "id": "SoMYZvUDyR1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mCCYvRqWyoCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***New Dataset***"
      ],
      "metadata": {
        "id": "ENfU-i_wzMDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_train, cifar_test = tff.simulation.datasets.cifar100.load_data()"
      ],
      "metadata": {
        "id": "X9wVsK8UzREs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cifar_train.client_ids)"
      ],
      "metadata": {
        "id": "XyUcV_ldzdVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_train.element_type_structure"
      ],
      "metadata": {
        "id": "Hhq31x8Hzvha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_dataset = cifar_train.create_tf_dataset_for_client(\n",
        "    cifar_train.client_ids[0])\n",
        "\n",
        "example_element = next(iter(example_dataset))\n",
        "\n",
        "example_element['label'].numpy()"
      ],
      "metadata": {
        "id": "m8jQhii4zvkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(example_element['image'].numpy(), cmap='gray', aspect='equal')\n",
        "plt.grid(False)\n",
        "_ = plt.show()"
      ],
      "metadata": {
        "id": "fxOFvQbyzvmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Example MNIST digits for one client\n",
        "figure = plt.figure(figsize=(20, 4))\n",
        "j = 0\n",
        "\n",
        "for example in example_dataset.take(50):\n",
        "  plt.subplot(5, 10, j+1)\n",
        "  plt.imshow(example['image'].numpy(), cmap='gray', aspect='equal')\n",
        "  plt.axis('off')\n",
        "  j += 1"
      ],
      "metadata": {
        "id": "A4kJYDJszvtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of examples per layer for a sample of clients\n",
        "f = plt.figure(figsize=(12, 7))\n",
        "f.suptitle('Label Counts for a Sample of Clients')\n",
        "for i in range(6):\n",
        "  client_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "      emnist_train.client_ids[i])\n",
        "  plot_data = collections.defaultdict(list)\n",
        "  for example in client_dataset:\n",
        "    # Append counts individually per label to make plots\n",
        "    # more colorful instead of one color per plot.\n",
        "    label = example['label'].numpy()\n",
        "    plot_data[label].append(label)\n",
        "  plt.subplot(2, 3, i+1)\n",
        "  plt.title('Client {}'.format(i))\n",
        "  for j in range(10):\n",
        "    plt.hist(\n",
        "        plot_data[j],\n",
        "        density=False,\n",
        "        bins=[i for i in range(100)])"
      ],
      "metadata": {
        "id": "Da9wEQ1TzvyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLIENTS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "  def batch_format_fn(element):\n",
        "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.reshape(element['image'], [-1, 61440]),\n",
        "        y=tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(\n",
        "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
      ],
      "metadata": {
        "id": "nmSKx9fpzv3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_example_dataset = preprocess(example_dataset)\n",
        "\n",
        "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
        "                                     next(iter(preprocessed_example_dataset)))\n",
        "\n",
        "sample_batch"
      ],
      "metadata": {
        "id": "RAUxxnFSzv8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in client_ids\n",
        "  ]"
      ],
      "metadata": {
        "id": "Ms8gPWUczv_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_clients = cifar_train.client_ids[0:NUM_CLIENTS]\n",
        "\n",
        "federated_train_data = make_federated_data(cifar_train, sample_clients)\n",
        "\n",
        "print(f'Number of client datasets: {len(federated_train_data)}')\n",
        "print(f'First dataset: {federated_train_data[0]}')"
      ],
      "metadata": {
        "id": "q8jjhiYbzwEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_keras_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(32, 32, 3)),\n",
        "      tf.keras.layers.Dense(64, kernel_initializer='zeros'),\n",
        "      tf.keras.layers.Softmax(100),\n",
        "  ])"
      ],
      "metadata": {
        "id": "EPkHkMZ_zwGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn():\n",
        "  # We _must_ create a new model here, and _not_ capture it from an external\n",
        "  # scope. TFF will call this within different graph contexts.\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=preprocessed_example_dataset.element_spec,\n",
        "      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "nLIuCtSKzwJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))"
      ],
      "metadata": {
        "id": "ECF4SEkxzwMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IC6fV8xOzwO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***cifar10 running***"
      ],
      "metadata": {
        "id": "a7jHY3mZUOhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import numpy as np\n",
        "import collections"
      ],
      "metadata": {
        "id": "O1mWcQ9dURNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 data\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Reduce the dataset to 30%\n",
        "train_size = int(0.3 * len(train_images))\n",
        "test_size = int(0.3 * len(test_images))\n",
        "\n",
        "train_images = train_images[:train_size]\n",
        "train_labels = train_labels[:train_size]\n",
        "test_images = test_images[:test_size]\n",
        "test_labels = test_labels[:test_size]\n",
        "\n",
        "# Function to preprocess the data\n",
        "def preprocess(dataset):\n",
        "    def batch_format_fn(element):\n",
        "        return collections.OrderedDict(\n",
        "            x=tf.cast(element['image'], tf.float32) / 255.0,  # Normalize the input images\n",
        "            y=tf.cast(element['label'], tf.int32))\n",
        "\n",
        "    return dataset.map(batch_format_fn).shuffle(100).batch(20)\n",
        "\n",
        "# Function to create federated data\n",
        "def make_federated_data(client_data, client_ids):\n",
        "    return [\n",
        "        preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "        for x in client_ids\n",
        "    ]"
      ],
      "metadata": {
        "id": "juAR9Pr3Ub-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create client data\n",
        "def create_client_data(images, labels, num_clients=10):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices({'image': images, 'label': labels})\n",
        "    client_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
        "        client_ids=[str(i) for i in range(num_clients)],\n",
        "        serializable_dataset_fn=lambda client_id: dataset.shard(num_clients, int(client_id)))\n",
        "    return client_data\n",
        "\n",
        "train_data = create_client_data(train_images, train_labels)\n",
        "test_data = create_client_data(test_images, test_labels)\n",
        "\n",
        "# Select a subset of clients for training\n",
        "client_ids = np.random.choice(train_data.client_ids, size=10, replace=False)\n",
        "federated_train_data = make_federated_data(train_data, client_ids)\n"
      ],
      "metadata": {
        "id": "9E0cKDIiUcB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_keras_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "JqUVXOMwUcD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn():\n",
        "    keras_model = create_keras_model()\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model,\n",
        "        input_spec=federated_train_data[0].element_spec,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n"
      ],
      "metadata": {
        "id": "CtjRGVaVUcH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn=model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n"
      ],
      "metadata": {
        "id": "EK-rRf3qUcK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = iterative_process.initialize()\n",
        "\n",
        "for round_num in range(1, 11):\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "    print(f'round {round_num}, metrics={metrics}')\n"
      ],
      "metadata": {
        "id": "agUJ3rg4UcSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **cifar10 new testing**"
      ],
      "metadata": {
        "id": "LXzMJk4ReADv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "VN1Nlz8DeFWl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(cifar_train_images, cifar_train_labels), (cifar_test_images, cifar_test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize the images to [0, 1] range\n",
        "cifar_train_images, cifar_test_images = cifar_train_images / 255.0, cifar_test_images / 255.0\n",
        "\n",
        "# Create a function to preprocess the data\n",
        "def preprocess(dataset, batch_size):\n",
        "    def batch_format_fn(element):\n",
        "        return (element['pixels'], element['label'])\n",
        "\n",
        "    dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "    return dataset.map(batch_format_fn).prefetch(10)\n",
        "\n",
        "def create_federated_data(client_data, client_ids, batch_size=32):\n",
        "    return [preprocess(client_data.create_tf_dataset_for_client(x), batch_size) for x in client_ids]\n",
        "\n",
        "# Define a function to convert numpy arrays to tf.data.Dataset\n",
        "def create_tf_dataset_for_client_fn(client_id):\n",
        "    client_id = int(client_id)\n",
        "    num_samples_per_client = 5000 // num_clients\n",
        "    start_index = client_id * num_samples_per_client\n",
        "    end_index = (client_id + 1) * num_samples_per_client\n",
        "    client_data = {\n",
        "        'pixels': cifar_train_images[start_index:end_index],\n",
        "        'label': cifar_train_labels[start_index:end_index]\n",
        "    }\n",
        "    return tf.data.Dataset.from_tensor_slices(client_data)\n",
        "\n",
        "# Create federated data\n",
        "num_clients = 10\n",
        "client_ids = [str(i) for i in range(num_clients)]\n",
        "client_data = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
        "    client_ids=client_ids,\n",
        "    serializable_dataset_fn=create_tf_dataset_for_client_fn\n",
        ")\n",
        "\n",
        "federated_train_data = create_federated_data(client_data, client_ids)"
      ],
      "metadata": {
        "id": "8RCua8CMeQL6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "def create_keras_model():\n",
        "          weight_decay = 1e-4\n",
        "          model = tf.keras.Sequential([\n",
        "              # 1st Block\n",
        "              layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), input_shape=(32, 32, 3), activation='relu', name=\"conv2d_1\"),\n",
        "              layers.BatchNormalization(name=\"batch_normalization_1\"),\n",
        "              layers.Dropout(0.3, name=\"dropout_1\"),\n",
        "              layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), activation='relu', name=\"conv2d_2\"),\n",
        "              layers.BatchNormalization(name=\"batch_normalization_2\"),\n",
        "              layers.MaxPooling2D(pool_size=(2, 2), name=\"max_pooling2d_1\"),\n",
        "\n",
        "              # 2nd Block\n",
        "              layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), activation='relu', name=\"conv2d_3\"),\n",
        "              layers.BatchNormalization(name=\"batch_normalization_3\"),\n",
        "              layers.Dropout(0.4, name=\"dropout_2\"),\n",
        "              layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), activation='relu', name=\"conv2d_4\"),\n",
        "              layers.BatchNormalization(name=\"batch_normalization_4\"),\n",
        "              layers.MaxPooling2D(pool_size=(2, 2), name=\"max_pooling2d_2\"),\n",
        "\n",
        "              # 3rdBlock\n",
        "              layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), activation='relu', name=\"conv2d_5\"),\n",
        "              layers.BatchNormalization(name=\"batch_normalization_5\"),\n",
        "              layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), activation='relu', name=\"conv2d_6\"),\n",
        "              layers.BatchNormalization(name=\"batch_normalization_6\"),\n",
        "              layers.MaxPooling2D(pool_size=(2, 2), name=\"max_pooling2d_3\"),\n",
        "              layers.Dropout(0.4, name=\"dropout_3\"),\n",
        "\n",
        "              # Fully Connected Layers\n",
        "              layers.Flatten(name=\"flatten_1\"),\n",
        "              layers.Dense(256, kernel_regularizer=tf.keras.regularizers.l2(weight_decay), activation='relu', name=\"dense_1\"),\n",
        "              layers.BatchNormalization(name=\"batch_normalization_7\"),\n",
        "              layers.Dropout(0.5, name=\"dropout_4\"),\n",
        "              layers.Dense(10, activation='softmax', name=\"dense_2\")\n",
        "          ])\n",
        "\n",
        "          return model\n",
        "\n",
        "def model_fn():\n",
        "    keras_model = create_keras_model()\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model,\n",
        "        input_spec=federated_train_data[0].element_spec,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n"
      ],
      "metadata": {
        "id": "-KPgQWjUeQOs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn=model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZV_KQ3CeQTK",
        "outputId": "72e89621-79b4-416f-936f-f1d25989e09e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_federated/python/learning/models/keras_utils.py:201: UserWarning: Batch Normalization contains non-trainable variables that won't be updated during the training. Consider using Group Normalization instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = iterative_process.initialize()\n",
        "\n",
        "for round_num in range(1, 11):  # Run for 10 rounds\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "    print(f'Round {round_num}, Metrics={metrics}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F74jkKB9eQV4",
        "outputId": "5375bc5f-256b-4d7e-f918-5f5c40c4ed05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1, Metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.1262), ('loss', 3.4419503), ('num_examples', 5000), ('num_batches', 160)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 2, Metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.1534), ('loss', 3.2115765), ('num_examples', 5000), ('num_batches', 160)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 3, Metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.1742), ('loss', 3.0204482), ('num_examples', 5000), ('num_batches', 160)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 4, Metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.1864), ('loss', 2.9495003), ('num_examples', 5000), ('num_batches', 160)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 5, Metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.2054), ('loss', 2.765757), ('num_examples', 5000), ('num_batches', 160)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 6, Metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.217), ('loss', 2.6824355), ('num_examples', 5000), ('num_batches', 160)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 7, Metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.2488), ('loss', 2.5628667), ('num_examples', 5000), ('num_batches', 160)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_data):\n",
        "    predictions = model.predict(test_data.map(lambda x, y: x))\n",
        "    y_true = np.concatenate([y for x, y in test_data], axis=0)\n",
        "    y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = np.mean(y_pred == y_true.squeeze())\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return accuracy, f1\n",
        "\n",
        "# Create and preprocess the test dataset\n",
        "test_data = tf.data.Dataset.from_tensor_slices((cifar_test_images, cifar_test_labels)).batch(32)\n",
        "\n",
        "# Load the final global model\n",
        "global_model = create_keras_model()\n",
        "# state.model.assign_weights_to(global_model)\n",
        "iterative_process.get_model_weights(state)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy, f1 = evaluate_model(global_model, test_data)\n",
        "print(f'Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "jv7UhVdZeQYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0BogQBrCeQaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yeiUF1kPeQcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tIpM4p10eQeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oD0a-7yUeQhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQpdG1-XeQj0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}